# Project Status Report — TMDB Movie Success Prediction

**Report Date:** February 28, 2026  
**Repository:** INSY 674 FinalProject  
**Course:** INSY 674

## 1. Current Project State
Project is in a complete end-to-end state with:
1. Data extraction and enrichment pipeline.
2. Feature engineering with leakage controls.
3. Supervised popularity modeling with comparison, cross-validation, ablation, and tuning.
4. Semi-supervised revenue-tier modeling with proper train/validation/test split (60/20/20).
5. Exported model artifacts for serving.
6. Streamlit app (`app/app_mockup.py`, `app/app_mockup2.py`) connected to trained models.

## 2. Scope Delivered
## 2.1 Data
1. Original input dataset: `data/movies_2010_2025.csv`.
2. Engineered and task-specific datasets generated by notebooks:
   - `data_features_master.csv`
   - `data_cleaned_engineered.csv`
   - `data_supervised_popularity.csv`
   - `data_supervised_revenue.csv`
   - `data_ssl_revenue.csv`
3. Saved model artifacts:
   - `data/best_ssl_model.joblib`
   - `data/ssl_scaler.joblib`
   - `data/ssl_model_comparison.csv`

## 2.2 Modeling
1. Popularity regression pipeline implemented in `models/PopularityModelComparison.ipynb`.
2. Revenue-tier SSL pipeline with full train/val/test methodology in `models/SemiSupervisedModels_Final.ipynb`.
3. Earlier SSL iterations retained for reference: `models/SemiSupervisedModels.ipynb` and `models/SemiSupervisedModels_V2.ipynb`.
4. Model export workflow implemented in `models/export_best_models.py`.

## 2.3 Application
1. App mockups: `app/app_mockup.py` and `app/app_mockup2.py`.
2. App uses exported model artifacts and metadata to run live scenario scoring.
3. App includes TMDB-assisted profile enrichment plus local fallbacks.

## 3. Repository Structure (Current)
```text
TMDB-Movies-INSY-674/
├── README.md
├── PROJECT_STATUS_REPORT.md
├── EDA/
│   └── EDA.ipynb
├── notebooks/
│   ├── DataExtraction.ipynb
│   └── FeatureEngineering.ipynb
├── data/
│   ├── movies_2010_2025.csv
│   ├── data_cleaned_engineered.csv
│   ├── data_features_master.csv
│   ├── data_supervised_popularity.csv
│   ├── data_supervised_revenue.csv
│   ├── data_ssl_revenue.csv
│   ├── ssl_model_comparison.csv
│   ├── best_ssl_model.joblib
│   └── ssl_scaler.joblib
├── models/
│   ├── PopularityModelComparison.ipynb
│   ├── SemiSupervisedModels.ipynb
│   ├── SemiSupervisedModels_V2.ipynb
│   ├── SemiSupervisedModels_Final.ipynb
│   └── export_best_models.py
└── app/
    ├── app_mockup.py
    └── app_mockup2.py
```

## 4. Work Completed by Pipeline Stage
## 4.1 Data Extraction (`notebooks/DataExtraction.ipynb`)
1. Pulled movie data across 2010-2025.
2. Enriched records with cast, director, and keyword metadata.
3. Produced `movies_2010_2025.csv`.

## 4.2 Feature Engineering (`notebooks/FeatureEngineering.ipynb`)
1. Created pre-release feature set (talent, content, temporal, production, text-derived).
2. Converted zero budget/revenue values to missing with indicator flags.
3. Preserved unlabeled revenue rows for SSL.
4. Built target-agnostic master plus supervised/SSL derived datasets.
5. Implemented leakage safeguards in model feature construction.

## 4.3 Popularity Modeling (`models/PopularityModelComparison.ipynb`)
1. Trained baseline, linear, tree, and boosting regressors.
2. Evaluated holdout metrics and K-Fold CV RMSE.
3. Ran repeated CV stability checks on top models.
4. Ran target ablation (`raw` vs `log1p`).
5. Ran XGBoost hyperparameter fine-tuning.
6. Added explainability outputs (feature importance + SHAP).

### Key metrics snapshot (from current notebook outputs)
1. Best raw-target holdout: XGBoost (`RMSE 4.185`, `MAE 1.626`, `R2 0.314`).
2. Best log-target setting: Gradient Boosting + log1p (`RMSE 3.507`, `MAE 1.420`, `R2 0.519`).
3. Tuned XGBoost improved over XGBoost baseline (`RMSE 4.149` vs `4.185`).

## 4.4 Semi-Supervised Revenue Tier Modeling (`models/SemiSupervisedModels_Final.ipynb`)
1. Built `y_ssl` tier labels (0=Low, 1=Medium, 2=High, 3=Blockbuster) with unlabeled rows as `-1`.
2. Applied leakage prevention (removed post-release features: vote, review, rating, revenue).
3. Implemented proper **60% train / 20% validation / 20% test** split on labeled data only.
   - `StandardScaler` fitted on train only; applied to val, test, and unlabeled.
   - PCA (95% variance) fitted on SSL training data only (used for graph-based models).
4. Hyperparameter tuning (RandomizedSearchCV) on train with CV — validation and test never touched during tuning.
5. Compared 5 models × base/tuned variants on **validation set**:
   - GradientBoosting (supervised, base + tuned)
   - RandomForest (supervised, base + tuned)
   - SelfTraining (SSL, base + tuned)
   - LabelSpreading (SSL, base + tuned)
   - LabelPropagation (SSL, base + tuned)
6. Selected best model by validation Macro F1 (precision → accuracy as tiebreaker).
7. Final unbiased evaluation performed **once** on **test set** (Step 6.5).
8. Exported summary results to `data/ssl_model_comparison.csv` and model artifacts.

### Train/Val/Test Roles
| Set | Size | Role |
|---|---|---|
| Train | 60% | Fit models, scaler, PCA, CV tuning |
| Validation | 20% | Model selection and threshold tuning |
| Test | 20% | One-time final unbiased evaluation |

## 4.5 Export and Packaging (`models/export_best_models.py`)
1. Recomputes best popularity model among candidate regressors under `log1p` policy.
2. Retrains chosen popularity model on full supervised popularity data.
3. Loads and repackages SSL model and scaler.
4. Writes metadata containing feature lists, target transform, and tier mapping.

Produced artifacts in `data/`:
1. `best_ssl_model.joblib`
2. `ssl_scaler.joblib`
3. `ssl_model_comparison.csv`

## 4.6 App (`app/app_mockup.py`, `app/app_mockup2.py`)
1. Uses exported artifacts for real inference (no mock scoring).
2. Supports scenario controls: director/cast + genre/language + runtime/release/budget/text proxies.
3. Predicts:
   - Popularity (back-transformed if model target is log1p)
   - Revenue tier
   - Confidence
   - Composite casting fit score
4. Displays TMDB/local “known-for” context for selected people.
5. Displays global feature importance chart from trained popularity model.

## 5. Validation and Robustness Covered
1. Holdout performance reporting on held-out test set.
2. K-Fold and repeated CV checks (popularity modeling).
3. Hyperparameter search — XGBoost (popularity) and RandomizedSearchCV (SSL).
4. Target-transform ablation (`raw` vs `log1p`).
5. Leakage-aware feature filtering for SSL (pre-release features only).
6. Train/validation/test separation — test set untouched until final evaluation.
7. Scaler and PCA fitted on training data only (no leakage to val/test/unlabeled).

## 6. Remaining Gaps / Risks
1. Random split and temporal drift can still cause performance variance.
2. Heavy-tailed target values increase sensitivity of RMSE.
3. App inference depends on strict feature parity with training metadata.
4. Live TMDB lookups can fail due to API/network limits (fallbacks included).

## 7. Recommended Next Steps
1. Add mandatory temporal CV to final model selection criteria.
2. Add prediction intervals or uncertainty bands in app outputs.
3. Add automated unit tests for feature-row generation and metadata compatibility.
4. Add model/version registry for reproducible deployment snapshots.
