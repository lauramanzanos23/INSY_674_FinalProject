# Project Status Report — TMDB Movie Success Prediction

**Report Date:** February 27, 2026  
**Repository:** INSY 674 FinalProject  
**Course:** INSY 674

## 1. Current Project State
Project is in a complete end-to-end state with:
1. Data extraction and enrichment pipeline.
2. Feature engineering with leakage controls.
3. Supervised popularity modeling with comparison, cross-validation, ablation, and tuning.
4. Semi-supervised revenue-tier modeling.
5. Exported model artifacts for serving.
6. Final Streamlit app (`app/app_final.py`) connected to trained models.

## 2. Scope Delivered
## 2.1 Data
1. Original input dataset: `data/movies_2010_2025.csv`.
2. Engineered and task-specific datasets generated by notebooks:
   - `data_features_master.csv`
   - `data_supervised_popularity.csv`
   - `data_supervised_revenue.csv`
   - `data_ssl_revenue.csv`

## 2.2 Modeling
1. Popularity regression pipeline implemented in `models/PopularityModelComparison.ipynb`.
2. Revenue-tier SSL pipeline implemented in `models/SemiSupervisedModels_V2.ipynb` (with earlier `SemiSupervisedModels.ipynb` retained).
3. Model export workflow implemented in `models/export_best_models.py`.

## 2.3 Application
1. Final app: `app/app_final.py`.
2. App uses exported model artifacts and metadata to run live scenario scoring.
3. App includes TMDB-assisted profile enrichment plus local fallbacks.

## 3. Repository Structure (Current)
```text
INSY 674 FinalProject/
├── README.md
├── PROJECT_STATUS_REPORT.md
├── EDA/
│   └── EDA.ipynb
├── notebooks/
│   ├── DataExtraction.ipynb
│   └── FeatureEngineering.ipynb
├── data/
│   └── movies_2010_2025.csv
├── models/
│   ├── PopularityModelComparison.ipynb
│   ├── SemiSupervisedModels.ipynb
│   ├── SemiSupervisedModels_V2.ipynb
│   └── export_best_models.py
├── app/
│   └── app_final.py
└── src/
```

## 4. Work Completed by Pipeline Stage
## 4.1 Data Extraction (`notebooks/DataExtraction.ipynb`)
1. Pulled movie data across 2010-2025.
2. Enriched records with cast, director, and keyword metadata.
3. Produced `movies_2010_2025.csv`.

## 4.2 Feature Engineering (`notebooks/FeatureEngineering.ipynb`)
1. Created pre-release feature set (talent, content, temporal, production, text-derived).
2. Converted zero budget/revenue values to missing with indicator flags.
3. Preserved unlabeled revenue rows for SSL.
4. Built target-agnostic master plus supervised/SSL derived datasets.
5. Implemented leakage safeguards in model feature construction.

## 4.3 Popularity Modeling (`models/PopularityModelComparison.ipynb`)
1. Trained baseline, linear, tree, and boosting regressors.
2. Evaluated holdout metrics and K-Fold CV RMSE.
3. Ran repeated CV stability checks on top models.
4. Ran target ablation (`raw` vs `log1p`).
5. Ran XGBoost hyperparameter fine-tuning.
6. Added explainability outputs (feature importance + SHAP).

### Key metrics snapshot (from current notebook outputs)
1. Best raw-target holdout: XGBoost (`RMSE 4.185`, `MAE 1.626`, `R2 0.314`).
2. Best log-target setting: Gradient Boosting + log1p (`RMSE 3.507`, `MAE 1.420`, `R2 0.519`).
3. Tuned XGBoost improved over XGBoost baseline (`RMSE 4.149` vs `4.185`).

## 4.4 Semi-Supervised Revenue Tier Modeling (`models/SemiSupervisedModels_V2.ipynb`)
1. Built `y_ssl` tier labels with unlabeled rows retained as `-1`.
2. Compared supervised and SSL classifiers.
3. Selected best model based on Macro F1 then Accuracy.
4. Exported summary results to `data/ssl_model_comparison.csv` and artifacts.

## 4.5 Export and Packaging (`models/export_best_models.py`)
1. Recomputes best popularity model among candidate regressors under `log1p` policy.
2. Retrains chosen popularity model on full supervised popularity data.
3. Loads and repackages SSL model and scaler.
4. Writes metadata containing feature lists, target transform, and tier mapping.

Produced artifacts in `models/`:
1. `popularity_best_model.pkl`
2. `ssl_best_model.pkl`
3. `ssl_scaler.pkl`
4. `model_metadata.pkl`

## 4.6 Final App (`app/app_final.py`)
1. Uses exported artifacts for real inference (no mock scoring).
2. Supports scenario controls: director/cast + genre/language + runtime/release/budget/text proxies.
3. Predicts:
   - Popularity (back-transformed if model target is log1p)
   - Revenue tier
   - Confidence
   - Composite casting fit score
4. Displays TMDB/local “known-for” context for selected people.
5. Displays global feature importance chart from trained popularity model.

## 5. Validation and Robustness Covered
1. Holdout performance reporting.
2. K-Fold and repeated CV checks.
3. Hyperparameter search (XGBoost).
4. Target-transform ablation.
5. Leakage-aware feature filtering for SSL.

## 6. Remaining Gaps / Risks
1. Random split and temporal drift can still cause performance variance.
2. Heavy-tailed target values increase sensitivity of RMSE.
3. App inference depends on strict feature parity with training metadata.
4. Live TMDB lookups can fail due to API/network limits (fallbacks included).

## 7. Recommended Next Steps
1. Add mandatory temporal CV to final model selection criteria.
2. Add prediction intervals or uncertainty bands in app outputs.
3. Add automated unit tests for feature-row generation and metadata compatibility.
4. Add model/version registry for reproducible deployment snapshots.
