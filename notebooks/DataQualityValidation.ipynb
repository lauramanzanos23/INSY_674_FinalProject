{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e6f9f08",
   "metadata": {},
   "source": [
    "# Data Quality Validation & Outlier Treatment (TMDB 2010-2025)\n",
    "\n",
    "This notebook performs **systematic data quality checks** and **outlier treatment** on the TMDB dataset\n",
    "before it enters the modeling pipeline.\n",
    "\n",
    "**Sections:**\n",
    "1. Schema & type validation\n",
    "2. Missing data analysis & visualization\n",
    "3. Outlier detection (IQR + z-score methods)\n",
    "4. Duplicate analysis\n",
    "5. Domain-specific validation rules\n",
    "6. Correlation-based feature cleanup\n",
    "7. Clean dataset export\n",
    "\n",
    "**Output:** `data/data_quality_validated.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136ab748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d26cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned/engineered dataset\n",
    "df = pd.read_csv(\"../data/data_cleaned_engineered.csv\")\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(f\"Rows: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3f0d4f",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Schema & Type Validation\n",
    "\n",
    "Verify that each column has the expected data type and flag any inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff13780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected types for key columns\n",
    "expected_types = {\n",
    "    \"movie_id\": \"numeric\",\n",
    "    \"title\": \"string\",\n",
    "    \"release_date\": \"datetime\",\n",
    "    \"runtime\": \"numeric\",\n",
    "    \"popularity\": \"numeric\",\n",
    "    \"vote_average\": \"numeric\",\n",
    "    \"vote_count\": \"numeric\",\n",
    "    \"budget\": \"numeric\",\n",
    "    \"revenue\": \"numeric\",\n",
    "    \"overview\": \"string\",\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SCHEMA VALIDATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "issues = []\n",
    "for col, expected in expected_types.items():\n",
    "    if col not in df.columns:\n",
    "        issues.append(f\"  MISSING: {col}\")\n",
    "        continue\n",
    "    \n",
    "    actual = df[col].dtype\n",
    "    status = \"OK\"\n",
    "    \n",
    "    if expected == \"numeric\" and not np.issubdtype(actual, np.number):\n",
    "        status = f\"WARN: expected numeric, got {actual}\"\n",
    "        issues.append(f\"  {col}: {status}\")\n",
    "    elif expected == \"datetime\" and actual == \"object\":\n",
    "        # Try to parse\n",
    "        try:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "            status = \"FIXED: converted to datetime\"\n",
    "        except:\n",
    "            status = \"WARN: could not convert to datetime\"\n",
    "            issues.append(f\"  {col}: {status}\")\n",
    "    \n",
    "    print(f\"  {col:20s} | dtype: {str(actual):15s} | {status}\")\n",
    "\n",
    "if issues:\n",
    "    print(f\"\\n{len(issues)} issue(s) found:\")\n",
    "    for issue in issues:\n",
    "        print(issue)\n",
    "else:\n",
    "    print(\"\\nAll schema checks passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb3ad7",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Missing Data Analysis\n",
    "\n",
    "Visualize missing data patterns to understand if data is MCAR, MAR, or MNAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77121b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data summary\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_report = pd.DataFrame({\n",
    "    \"missing_count\": missing,\n",
    "    \"missing_pct\": missing_pct\n",
    "}).sort_values(\"missing_pct\", ascending=False)\n",
    "\n",
    "# Show only columns with missing values\n",
    "missing_report = missing_report[missing_report[\"missing_count\"] > 0]\n",
    "print(f\"Columns with missing values: {len(missing_report)} / {len(df.columns)}\")\n",
    "print(missing_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a1439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data patterns\n",
    "if len(missing_report) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Bar chart of missing percentages\n",
    "    top_missing = missing_report.head(15)\n",
    "    axes[0].barh(top_missing.index, top_missing[\"missing_pct\"], color=\"coral\")\n",
    "    axes[0].set_xlabel(\"Missing %\")\n",
    "    axes[0].set_title(\"Top 15 Columns by Missing Data %\")\n",
    "    axes[0].invert_yaxis()\n",
    "    \n",
    "    # Missing data heatmap (sample for readability)\n",
    "    cols_with_missing = missing_report.head(10).index.tolist()\n",
    "    if cols_with_missing:\n",
    "        sample = df[cols_with_missing].sample(min(200, len(df)), random_state=42)\n",
    "        sns.heatmap(sample.isnull(), cbar=True, yticklabels=False, ax=axes[1],\n",
    "                    cmap=\"YlOrRd\")\n",
    "        axes[1].set_title(\"Missing Data Pattern (sample of rows)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No missing values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b726a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data correlation: are columns missing together?\n",
    "cols_with_missing = df.columns[df.isnull().any()].tolist()\n",
    "\n",
    "if len(cols_with_missing) > 1:\n",
    "    missing_corr = df[cols_with_missing].isnull().corr()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(missing_corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\",\n",
    "                center=0, ax=ax, vmin=-1, vmax=1)\n",
    "    ax.set_title(\"Missing Data Correlation Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find highly correlated missing patterns\n",
    "    high_corr = []\n",
    "    for i in range(len(missing_corr.columns)):\n",
    "        for j in range(i+1, len(missing_corr.columns)):\n",
    "            if abs(missing_corr.iloc[i, j]) > 0.5:\n",
    "                high_corr.append((missing_corr.columns[i], missing_corr.columns[j], missing_corr.iloc[i, j]))\n",
    "    \n",
    "    if high_corr:\n",
    "        print(\"Columns that tend to be missing together (corr > 0.5):\")\n",
    "        for c1, c2, corr in high_corr:\n",
    "            print(f\"  {c1} <-> {c2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"Not enough columns with missing data for correlation analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c68c3e",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Outlier Detection & Treatment\n",
    "\n",
    "Identify outliers using both **IQR** and **z-score** methods. We apply treatment strategies\n",
    "appropriate for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc08794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(series, multiplier=1.5):\n",
    "    \"\"\"Detect outliers using the IQR method.\"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - multiplier * IQR\n",
    "    upper = Q3 + multiplier * IQR\n",
    "    outliers = (series < lower) | (series > upper)\n",
    "    return outliers, lower, upper\n",
    "\n",
    "def detect_outliers_zscore(series, threshold=3):\n",
    "    \"\"\"Detect outliers using z-score method.\"\"\"\n",
    "    z = np.abs((series - series.mean()) / series.std())\n",
    "    return z > threshold\n",
    "\n",
    "# Key numeric columns to check for outliers\n",
    "outlier_cols = [\"runtime\", \"budget\", \"revenue\", \"popularity\", \"vote_average\", \n",
    "                \"vote_count\", \"director_popularity\", \"cast_pop_mean\"]\n",
    "outlier_cols = [c for c in outlier_cols if c in df.columns]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"OUTLIER DETECTION REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "outlier_summary = []\n",
    "for col in outlier_cols:\n",
    "    series = df[col].dropna()\n",
    "    if len(series) == 0:\n",
    "        continue\n",
    "    \n",
    "    iqr_mask, lower, upper = detect_outliers_iqr(series)\n",
    "    zscore_mask = detect_outliers_zscore(series)\n",
    "    \n",
    "    outlier_summary.append({\n",
    "        \"column\": col,\n",
    "        \"n_valid\": len(series),\n",
    "        \"iqr_outliers\": iqr_mask.sum(),\n",
    "        \"iqr_pct\": f\"{iqr_mask.mean()*100:.1f}%\",\n",
    "        \"zscore_outliers\": zscore_mask.sum(),\n",
    "        \"zscore_pct\": f\"{zscore_mask.mean()*100:.1f}%\",\n",
    "        \"iqr_lower\": f\"{lower:.2f}\",\n",
    "        \"iqr_upper\": f\"{upper:.2f}\",\n",
    "        \"min\": f\"{series.min():.2f}\",\n",
    "        \"max\": f\"{series.max():.2f}\",\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "print(outlier_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edae49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions with outlier thresholds\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(outlier_cols[:8]):\n",
    "    ax = axes[i]\n",
    "    series = df[col].dropna()\n",
    "    \n",
    "    # Box plot\n",
    "    bp = ax.boxplot(series, vert=True, patch_artist=True,\n",
    "                    boxprops=dict(facecolor='lightblue', color='navy'),\n",
    "                    medianprops=dict(color='red', linewidth=2))\n",
    "    ax.set_title(col, fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel(\"Value\")\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(len(outlier_cols), len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.suptitle(\"Box Plots \\u2014 Outlier Visualization\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c758dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply outlier treatment: cap extreme values at IQR boundaries (winsorization)\n",
    "# Only applied to columns where capping makes domain sense\n",
    "\n",
    "cap_columns = [\"runtime\", \"budget\", \"revenue\", \"popularity\", \"vote_count\",\n",
    "               \"director_popularity\", \"cast_pop_mean\"]\n",
    "cap_columns = [c for c in cap_columns if c in df.columns]\n",
    "\n",
    "cap_report = []\n",
    "for col in cap_columns:\n",
    "    before_min, before_max = df[col].min(), df[col].max()\n",
    "    _, lower, upper = detect_outliers_iqr(df[col].dropna(), multiplier=3.0)  # Use 3x IQR (conservative)\n",
    "    \n",
    "    n_capped_low = (df[col] < lower).sum()\n",
    "    n_capped_high = (df[col] > upper).sum()\n",
    "    \n",
    "    df[f\"{col}_capped\"] = df[col].clip(lower=lower, upper=upper)\n",
    "    \n",
    "    cap_report.append({\n",
    "        \"column\": col,\n",
    "        \"capped_low\": n_capped_low,\n",
    "        \"capped_high\": n_capped_high,\n",
    "        \"cap_lower\": f\"{lower:.0f}\",\n",
    "        \"cap_upper\": f\"{upper:.0f}\",\n",
    "    })\n",
    "\n",
    "print(\"Winsorization report (3x IQR):\")\n",
    "print(pd.DataFrame(cap_report).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e69a08",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Duplicate Analysis\n",
    "\n",
    "Check for exact and near-duplicate movies that could bias the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370057ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact duplicates by movie_id\n",
    "if \"movie_id\" in df.columns:\n",
    "    exact_dupes = df.duplicated(subset=[\"movie_id\"], keep=False)\n",
    "    print(f\"Exact duplicate movie_ids: {exact_dupes.sum()}\")\n",
    "    if exact_dupes.sum() > 0:\n",
    "        print(df[exact_dupes][[\"movie_id\", \"title\", \"release_date\"]].sort_values(\"movie_id\"))\n",
    "\n",
    "# Near-duplicates: same title + same year\n",
    "if \"title\" in df.columns and \"release_year\" in df.columns:\n",
    "    near_dupes = df.duplicated(subset=[\"title\", \"release_year\"], keep=False)\n",
    "    n_near = near_dupes.sum()\n",
    "    print(f\"\\nNear-duplicates (same title + year): {n_near}\")\n",
    "    if n_near > 0 and n_near < 50:\n",
    "        print(df[near_dupes][[\"movie_id\", \"title\", \"release_year\", \"popularity\"]]\n",
    "              .sort_values([\"title\", \"release_year\"]).head(20))\n",
    "    elif n_near >= 50:\n",
    "        print(f\"  (showing first 20)\")\n",
    "        print(df[near_dupes][[\"movie_id\", \"title\", \"release_year\", \"popularity\"]]\n",
    "              .sort_values([\"title\", \"release_year\"]).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596367bf",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Domain-Specific Validation Rules\n",
    "\n",
    "Apply business logic checks that catch data quality issues specific to the movie domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6feae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DOMAIN VALIDATION RULES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "validations = []\n",
    "\n",
    "# Rule 1: Runtime should be between 1 and 600 minutes\n",
    "if \"runtime\" in df.columns:\n",
    "    bad_runtime = (df[\"runtime\"].notna()) & ((df[\"runtime\"] < 1) | (df[\"runtime\"] > 600))\n",
    "    validations.append((\"Runtime outside 1-600 min\", bad_runtime.sum()))\n",
    "\n",
    "# Rule 2: Vote average should be between 0 and 10\n",
    "if \"vote_average\" in df.columns:\n",
    "    bad_votes = (df[\"vote_average\"].notna()) & ((df[\"vote_average\"] < 0) | (df[\"vote_average\"] > 10))\n",
    "    validations.append((\"Vote average outside 0-10\", bad_votes.sum()))\n",
    "\n",
    "# Rule 3: Budget should not be negative\n",
    "if \"budget\" in df.columns:\n",
    "    neg_budget = (df[\"budget\"].notna()) & (df[\"budget\"] < 0)\n",
    "    validations.append((\"Negative budget\", neg_budget.sum()))\n",
    "\n",
    "# Rule 4: Revenue should not be negative\n",
    "if \"revenue\" in df.columns:\n",
    "    neg_revenue = (df[\"revenue\"].notna()) & (df[\"revenue\"] < 0)\n",
    "    validations.append((\"Negative revenue\", neg_revenue.sum()))\n",
    "\n",
    "# Rule 5: Release year within expected range\n",
    "if \"release_year\" in df.columns:\n",
    "    bad_year = (df[\"release_year\"].notna()) & ((df[\"release_year\"] < 2010) | (df[\"release_year\"] > 2025))\n",
    "    validations.append((\"Release year outside 2010-2025\", bad_year.sum()))\n",
    "\n",
    "# Rule 6: Popularity should be non-negative\n",
    "if \"popularity\" in df.columns:\n",
    "    neg_pop = (df[\"popularity\"].notna()) & (df[\"popularity\"] < 0)\n",
    "    validations.append((\"Negative popularity\", neg_pop.sum()))\n",
    "\n",
    "# Rule 7: ROI sanity check (unrealistically high ROI > 1000x)\n",
    "if \"roi\" in df.columns:\n",
    "    extreme_roi = (df[\"roi\"].notna()) & (df[\"roi\"] > 1000)\n",
    "    validations.append((\"Extreme ROI > 1000x\", extreme_roi.sum()))\n",
    "\n",
    "# Rule 8: Revenue without budget (potential data issue)\n",
    "if \"budget\" in df.columns and \"revenue\" in df.columns:\n",
    "    rev_no_budget = (df[\"revenue\"].notna()) & (df[\"revenue\"] > 0) & (df[\"budget\"].isna())\n",
    "    validations.append((\"Revenue present but budget missing\", rev_no_budget.sum()))\n",
    "\n",
    "for rule, count in validations:\n",
    "    status = \"PASS\" if count == 0 else f\"FAIL ({count} rows)\"\n",
    "    icon = \"OK\" if count == 0 else \"!!\"\n",
    "    print(f\"  [{icon}] {rule:45s} -> {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376cdc14",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Feature Correlation Analysis\n",
    "\n",
    "Identify highly correlated features that may cause multicollinearity in models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c70deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix for numeric features\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "# Find highly correlated pairs (|r| > 0.85)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        r = corr_matrix.iloc[i, j]\n",
    "        if abs(r) > 0.85:\n",
    "            high_corr_pairs.append({\n",
    "                \"feature_1\": corr_matrix.columns[i],\n",
    "                \"feature_2\": corr_matrix.columns[j],\n",
    "                \"correlation\": round(r, 3)\n",
    "            })\n",
    "\n",
    "high_corr_df = pd.DataFrame(high_corr_pairs).sort_values(\"correlation\", ascending=False)\n",
    "print(f\"Feature pairs with |correlation| > 0.85: {len(high_corr_df)}\")\n",
    "if len(high_corr_df) > 0:\n",
    "    print(high_corr_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cbfb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for key features\n",
    "key_features = [\"runtime\", \"popularity\", \"vote_average\", \"vote_count\", \"budget\", \n",
    "                \"revenue\", \"director_popularity\", \"cast_pop_mean\", \"genres_count\",\n",
    "                \"keywords_count\", \"overview_word_count\", \"roi\"]\n",
    "key_features = [c for c in key_features if c in numeric_df.columns]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(numeric_df[key_features].corr(), annot=True, fmt=\".2f\",\n",
    "            cmap=\"RdBu_r\", center=0, ax=ax, vmin=-1, vmax=1,\n",
    "            square=True, linewidths=0.5)\n",
    "ax.set_title(\"Feature Correlation Heatmap (Key Variables)\", fontsize=13, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d8054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggest features to drop based on high correlation\n",
    "# Strategy: for each highly correlated pair, suggest dropping the one with lower\n",
    "# correlation to the target (popularity)\n",
    "\n",
    "if \"popularity\" in numeric_df.columns and len(high_corr_df) > 0:\n",
    "    target_corr = numeric_df.corr()[\"popularity\"].abs()\n",
    "    \n",
    "    drop_candidates = set()\n",
    "    keep_reasons = []\n",
    "    \n",
    "    for _, row in high_corr_df.iterrows():\n",
    "        f1, f2 = row[\"feature_1\"], row[\"feature_2\"]\n",
    "        if f1 == \"popularity\" or f2 == \"popularity\":\n",
    "            continue\n",
    "        \n",
    "        corr1 = target_corr.get(f1, 0)\n",
    "        corr2 = target_corr.get(f2, 0)\n",
    "        \n",
    "        if corr1 >= corr2:\n",
    "            drop_candidates.add(f2)\n",
    "            keep_reasons.append(f\"  Keep {f1} (|r|={corr1:.3f}) over {f2} (|r|={corr2:.3f})\")\n",
    "        else:\n",
    "            drop_candidates.add(f1)\n",
    "            keep_reasons.append(f\"  Keep {f2} (|r|={corr2:.3f}) over {f1} (|r|={corr1:.3f})\")\n",
    "    \n",
    "    print(f\"\\nSuggested features to drop ({len(drop_candidates)}):\")\n",
    "    for feat in sorted(drop_candidates):\n",
    "        print(f\"  - {feat}\")\n",
    "    print(\"\\nReasoning:\")\n",
    "    for reason in keep_reasons:\n",
    "        print(reason)\n",
    "else:\n",
    "    print(\"No highly correlated feature pairs found, or no target column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5472ce38",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Data Quality Summary & Export\n",
    "\n",
    "Generate a final quality report and export the validated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c688b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FINAL DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_rows = len(df)\n",
    "total_cols = len(df.columns)\n",
    "total_cells = total_rows * total_cols\n",
    "total_missing = df.isnull().sum().sum()\n",
    "completeness = (1 - total_missing / total_cells) * 100\n",
    "\n",
    "print(f\"\\n  Rows:           {total_rows:,}\")\n",
    "print(f\"  Columns:        {total_cols}\")\n",
    "print(f\"  Total cells:    {total_cells:,}\")\n",
    "print(f\"  Missing cells:  {total_missing:,}\")\n",
    "print(f\"  Completeness:   {completeness:.2f}%\")\n",
    "\n",
    "# Duplicate count\n",
    "if \"movie_id\" in df.columns:\n",
    "    n_dupes = df.duplicated(subset=[\"movie_id\"]).sum()\n",
    "    print(f\"  Duplicates:     {n_dupes}\")\n",
    "\n",
    "# Data type breakdown\n",
    "dtype_counts = df.dtypes.value_counts()\n",
    "print(\"\\n  Data type breakdown:\")\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f\"    {str(dtype):20s}: {count} columns\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1362c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export validated dataset\n",
    "out_path = \"../data/data_quality_validated.csv\"\n",
    "df.to_csv(out_path, index=False)\n",
    "print(f\"Saved validated dataset to: {out_path}\")\n",
    "print(f\"Final shape: {df.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
